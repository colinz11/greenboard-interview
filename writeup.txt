My thought process for tackling this project. 

First, code was written using Kiro, amazon's coding IDE, as there was clearly given specifications with success criteria. A task that Kiro excels at.

Given this was a MVP/POC and scaling to production is not a priority I used the technologies that are easiest to develop in and get working code to run.

Initially, the first question would be the data model and what data storage methods to use. However, given the time constraints and ease, I decided to stick to just using files and storing in the backend in folders. 

Starting with backend, the first decisions was how does the API structure need to work in order to display the necessary data on the front end. A basic list would have to include ability to archive, view an archive by id, track versioning for an archive by id, and delete an archive. A simple API structure is best for an initial mock up design, but this design doesnâ€™t account for different users, the ability to login and see archives for a specific user. From a services standpoint it makes sense to have a general archiving service that will handling all logic from when a user sends an archive request to saving the archive itself. I also decided to have a separate crawler service as this is the bulk of bottleneck and will take the most time. In the future, given a micro services architecture this could be scaled independently and used by other services. I also decided to use a file service to save and track the versions and assign ids to each archive. It makes sense as this is something that should be changed in the future to use a database so we want modularity here and abstraction. 

From a UI perspective, I tried to keep it as simple as possible. All of the UI code was generated by AI, and I will admit in its current form it is not scalable or modular. I chose this because it was the fastest and easiest to get something working. But in the future, there would need to be globalized css styles, components, pages and a data model to match the back end.

In terms of production scalability. The biggest thing is a database and file storage system. One option is S3 for storing files and any relational databases to store meta data related to the files or necessary for the UI. Another would be a need for a queue or some load balancer in terms of the crawler. If there are many users trying to archive many pages, the backend would need some way to balance the requests. These I think would be the two biggest necessities. For production, you also need the basics CI/CD pipeline with tests on the backend.